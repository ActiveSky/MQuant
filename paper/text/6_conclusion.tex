\vspace{-2mm}
\section{Conclusion} \label{sec:conclusion}
% \vspace{-1mm}
In this paper, we propose \emph{MQuant}, an accurate and efficient PTQ framework specifically designed for MLLMs. Our approach addresses the challenges of MLLMs by applying \textbf{Modality-Specific Static Quantization (MSQ)} to handle distribution mismatches between visual and text tokens, and introducing an \textbf{Attention-Invariant Flexible Switching (AIFS)} mechanism to reduce TTFT with static per-tensor scaling. Furthermore, we reveal that online Hadamard rotations can lead to weight outliers, and propose \textbf{Rotation Magnitude Suppression (RMS)} to mitigate this issue. Extensive experiments on 5 mainstream MLLMs demonstrate that \emph{MQuant} achieves near-floating-point accuracy under the W4A8 setting, highlighting its potential to advance MLLMs quantization and practical deployment in resource-constrained environments. For the quantization of video-based MLLMs, we also discuss in Appendix~\ref{video_MLLMs}.